# RL-Control-Suite ğŸ¯

[![Tests](https://github.com/yourusername/rl-control-suite/workflows/Tests/badge.svg)](https://github.com/yourusername/rl-control-suite/actions)
[![Coverage](https://img.shields.io/badge/coverage-95%25-brightgreen.svg)](https://github.com/yourusername/rl-control-suite)
[![Documentation](https://readthedocs.org/projects/rl-control-suite/badge/?version=latest)](https://rl-control-suite.readthedocs.io/)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Production-ready Python library combining reinforcement learning and optimal control with formal safety guarantees.

## ğŸš€ Key Features

- **Hybrid RL Algorithms**: PPO, SAC, and Model Predictive Control implementations
- **Safety-Critical Control**: Formal verification methods for safety guarantees via Control Barrier Functions
- **Production-Ready**: 95%+ test coverage, comprehensive documentation, and CI/CD
- **Extensible Architecture**: Plugin system for custom algorithms
- **Performance Optimized**: JAX acceleration and efficient replay buffers

## ğŸ“¦ Installation

### Basic Installation
```bash
pip install rl-control-suite
```

### Development Installation
```bash
git clone https://github.com/yourusername/rl-control-suite.git
cd rl-control-suite
pip install -e ".[dev,docs]"
```

### With Optional Dependencies
```bash
# For Model Predictive Control
pip install "rl-control-suite[mpc]"

# For JAX acceleration
pip install "rl-control-suite[jax]"

# All optional dependencies
pip install "rl-control-suite[mpc,jax,dev,docs]"
```

### Docker Installation
```bash
docker build -t rl-control-suite .
docker run -it rl-control-suite
```

## ğŸ“ Quick Start

### Training with PPO
```python
from rl_control.algorithms import PPO
from rl_control.envs import make_env

# Create environment
env = make_env("CartPole-v1")

# Initialize PPO agent
agent = PPO(
    state_dim=env.observation_space.shape[0],
    action_dim=env.action_space.n,
    hidden_dims=[64, 64],
    lr=3e-4
)

# Train the agent
agent.train(env, total_timesteps=100000)

# Save the model
agent.save("models/ppo_cartpole")
```

### Training with SAC (Continuous Control)
```python
from rl_control.algorithms import SAC
from rl_control.envs import make_env

env = make_env("Pendulum-v1")

agent = SAC(
    state_dim=env.observation_space.shape[0],
    action_dim=env.action_space.shape[0],
    hidden_dims=[256, 256],
    lr=3e-4
)

agent.train(env, total_timesteps=50000)
```

### Model Predictive Control
```python
from rl_control.algorithms import MPC
from rl_control.envs import make_env
import numpy as np

env = make_env("Pendulum-v1")

# Define cost function
def cost_fn(state, action):
    angle, velocity = state[0], state[1]
    cost = angle**2 + 0.1 * velocity**2 + 0.001 * action**2
    return cost

# Initialize MPC controller
mpc = MPC(
    state_dim=3,
    action_dim=1,
    horizon=20,
    cost_function=cost_fn,
    dynamics_model=None  # Will use learned model
)

# Run MPC
state = env.reset()
for _ in range(100):
    action = mpc.compute_action(state)
    state, reward, done, info = env.step(action)
```

### Safe Control with Barrier Functions
```python
from rl_control.safety import ControlBarrierFunction
from rl_control.algorithms import SAC
import numpy as np

# Define safety constraint (e.g., position limit)
def barrier_function(state):
    position = state[0]
    return 1.0 - (position / 2.0)**2  # Safe when h(x) > 0

cbf = ControlBarrierFunction(
    barrier_fn=barrier_function,
    alpha=1.0  # Class-K function parameter
)

# Wrap SAC agent with safety filter
agent = SAC(state_dim=3, action_dim=1)
safe_agent = cbf.wrap_agent(agent)

# Now agent actions are filtered to ensure safety
state = env.reset()
action = safe_agent.get_action(state)  # Guaranteed safe!
```

## ğŸ“š Documentation

Full documentation is available at [Read the Docs](https://rl-control-suite.readthedocs.io/).

### Tutorials
- [Getting Started](docs/tutorials/getting_started.md)
- [Training Custom Agents](docs/tutorials/custom_agents.md)
- [Safety-Critical Control](docs/tutorials/safety.md)
- [EV Charging Optimization](docs/tutorials/ev_optimization.md)

## ğŸ§ª Testing

Run the full test suite:
```bash
pytest tests/ -v --cov
```

Run specific test categories:
```bash
# Unit tests only
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# With coverage report
pytest --cov=src/rl_control --cov-report=html
```

## ğŸ—ï¸ Project Structure

```
rl-control-suite/
â”œâ”€â”€ src/rl_control/          # Main library code
â”‚   â”œâ”€â”€ algorithms/          # RL and control algorithms
â”‚   â”œâ”€â”€ safety/              # Safety verification tools
â”‚   â”œâ”€â”€ envs/                # Environment wrappers
â”‚   â””â”€â”€ utils/               # Utilities (logging, buffers)
â”œâ”€â”€ tests/                   # Test suite (95%+ coverage)
â”œâ”€â”€ docs/                    # Sphinx documentation
â”œâ”€â”€ examples/                # Example scripts
â””â”€â”€ .github/workflows/       # CI/CD pipelines
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- PPO implementation inspired by OpenAI Spinning Up
- SAC based on original Berkeley research
- MPC formulation using CasADi optimization framework
- Safety verification methods from control barrier function literature

## ğŸ“§ Contact

- **Author**: Your Name
- **Email**: your.email@example.com
- **GitHub**: [@yourusername](https://github.com/yourusername)

## ğŸŒŸ Citation

If you use this library in your research, please cite:

```bibtex
@software{rl_control_suite2024,
  author = {Your Name},
  title = {RL-Control-Suite: Production-Ready Reinforcement Learning and Optimal Control},
  year = {2024},
  url = {https://github.com/yourusername/rl-control-suite}
}
```

## ğŸ“Š Performance Benchmarks

| Algorithm | Environment | Mean Reward | Training Time |
|-----------|-------------|-------------|---------------|
| PPO       | CartPole-v1 | 500 Â± 0     | 2 min         |
| SAC       | Pendulum-v1 | -150 Â± 20   | 5 min         |
| MPC       | Pendulum-v1 | -180 Â± 15   | 1 min         |

---

**Built with â¤ï¸ for the RL and control community**
